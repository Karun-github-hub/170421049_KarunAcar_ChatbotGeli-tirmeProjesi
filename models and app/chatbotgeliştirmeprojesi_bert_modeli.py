# -*- coding: utf-8 -*-
"""ChatbotGeliÅŸtirmeProjesi-BERT-Modeli.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WlgAuPNcpXTzChU4s9UExRhP7DpHKQ2t
"""

!pip install -r /content/drive/MyDrive/requirements.txt

import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset, load_metric
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import numpy as np

# 1. Veri yÃ¼kleme ve hazÄ±rlama
df = pd.read_excel("/content/drive/MyDrive/university_intents_1000.xlsx")

# Label encoding
le = LabelEncoder()
df['label'] = le.fit_transform(df['Intent'])

# EÄŸitim test bÃ¶lme
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['Example'].tolist(),
    df['label'].tolist(),
    test_size=0.2,
    stratify=df['label'],
    random_state=42
)

# Tokenizer ve Model
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Tokenize etme fonksiyonu
def tokenize(batch):
    return tokenizer(batch, padding=True, truncation=True, max_length=128)

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)



class IntentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item



train_dataset = IntentDataset(train_encodings, train_labels)
val_dataset = IntentDataset(val_encodings, val_labels)

# Model yÃ¼kleme
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(le.classes_))

# DeÄŸerlendirme metriÄŸi
accuracy_metric = load_metric("accuracy")
precision_metric = load_metric("precision")
recall_metric = load_metric("recall")
f1_metric = load_metric("f1")


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    precision = precision_score(labels, predictions, average='weighted')
    recall = recall_score(labels, predictions, average='weighted')
    f1 = f1_score(labels, predictions, average='weighted')
    accuracy = accuracy_score(labels, predictions)
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

# EÄŸitim parametreleri
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=10,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model='f1',
    greater_is_better=True,
    seed=42
)

# Trainer oluÅŸturma
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

# EÄŸitim
trainer.train()

# DeÄŸerlendirme
eval_results = trainer.evaluate()

print("\nDeÄŸerlendirme SonuÃ§larÄ±:")
for key, value in eval_results.items():
    if key.startswith("eval_"):
        print(f"{key}: {value:.4f}")

# Confusion Matrix oluÅŸturma
val_logits = trainer.predict(val_dataset).predictions
val_preds = np.argmax(val_logits, axis=-1)

cm = confusion_matrix(val_labels, val_preds)
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Tahmin Edilen SÄ±nÄ±f')
plt.ylabel('GerÃ§ek SÄ±nÄ±f')
plt.title('Confusion Matrix')
plt.show()

# Model kaydetme
model.save_pretrained("models/bert_intent_model")
tokenizer.save_pretrained("models/bert_intent_model")
print("Model kaydedildi: models/bert_intent_model")

!pip install streamlit

!pip install streamlit pyngrok

!ngrok authtoken 2xNKYsZksXHwuaKnB16y5x5vwFN_34LoxPuUPHqPRz7RYJzoj

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import os
# import torch
# from transformers import AutoTokenizer, AutoModelForSequenceClassification
# import openai
# import numpy as np
# from openai import OpenAI
# import os
# from openai import OpenAI
# 
# 
# 
# import os
# os.environ["OPENAI_API_KEY"] = "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"
# 
# from openai import OpenAI
# 
# client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
# 
# 
# 
# # Model ve tokenizer yÃ¼kle
# model_path = "models/bert_intent_model"
# tokenizer = AutoTokenizer.from_pretrained(model_path)
# model = AutoModelForSequenceClassification.from_pretrained(model_path)
# model.eval()
# 
# # Intent sÄ±nÄ±flarÄ± (modelin eÄŸitildiÄŸi sÄ±rayla aynÄ± olmalÄ±)
# intent_labels = [ "AcademicCalender" ,"ContactInfo", "DepartmentInfo", "DormitoryInfo","EventInfo", "Goodbye",
#     "Greeting", "Registration","Reject","ScholarshipInfo"
# ]
# 
# def predict_intent(text):
#     inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
#     with torch.no_grad():
#         outputs = model(**inputs)
#         logits = outputs.logits
#         predicted_class_id = logits.argmax().item()
#     return intent_labels[predicted_class_id]
# 
# def generate_gpt_response(intent, user_text):
#     messages = [
#         {"role": "system", "content": "Sen Ã¼niversite chatbotusun."},
#         {"role": "user", "content": f"KullanÄ±cÄ±nÄ±n niyeti: {intent}. KullanÄ±cÄ±nÄ±n sorusu: {user_text} Bu niyete uygun, kÄ±sa ve net cevap ver."}
#     ]
#     response = client.chat.completions.create(
#         model="gpt-3.5-turbo",
#         messages=messages,
#         max_tokens=150,
#         temperature=0.3,
#     )
#     return response.choices[0].message.content.strip()
# 
# # ------------------ Sayfa AyarlarÄ± --------------------
# st.set_page_config(
#     page_title="ğŸ“ Ãœniversite Destek Chatbotu"
# 
# )
# 
# 
# # ------------------ BaÅŸlÄ±k --------------------
# col1, col2 = st.columns([1, 9])
# with col1:
#     st.image("https://cdn-icons-png.flaticon.com/512/8943/8943377.png", width=64)
# with col2:
#     st.title("Karun'un Ãœniversite Destek Chatbotu")
#     st.caption("ğŸ¤– BERT + GPT-3.5 destekli akÄ±llÄ± asistan")
# 
# # ------------------ KullanÄ±cÄ± GiriÅŸi --------------------
# user_input = st.text_input("âœï¸ LÃ¼tfen sorunuzu yazÄ±nÄ±z:")
# 
# if user_input:
#     with st.spinner("ğŸ” YanÄ±t hazÄ±rlanÄ±yor..."):
#         intent = predict_intent(user_input.lower())
#         answer = generate_gpt_response(intent, user_input)
# 
#     # ------------------ YanÄ±t GÃ¶sterimi --------------------
#     st.markdown(f"#### ğŸ¯ Tahmin Edilen Niyet: `{intent}`")
#     st.markdown(f"<div class='chatbox'>ğŸ’¬ <b>Chatbot:</b> {answer}</div>", unsafe_allow_html=True)
#

import os
from pyngrok import ngrok
os.system("streamlit run app.py --server.port 8501 &")
public_url = ngrok.connect(8501)
print("Public URL:", public_url)